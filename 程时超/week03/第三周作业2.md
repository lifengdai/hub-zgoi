|                             模型                             |                             优点                             | 缺点                                                         | 适用场景                                   |
| :----------------------------------------------------------: | :----------------------------------------------------------: | ------------------------------------------------------------ | ------------------------------------------ |
| BERT（Bidirectional Encoder Representations from Transformers） |             拥有强大的语言表征能力和特征提取能力             | 模型收敛较慢，需要强大的算力支撑；需要较大的数据集进行训练，对计算资源要求较高 | 适用于复杂语义和高精度要求场景             |
|          GPT（Generative Pre-trained Transformers）          | 拥有强大的文本生成能力和语言理解能力；无需训练，可直接利用预训练大模型 | 训练过程中需要大量的计算资源和时间；容易产生重复和无意义的内容 | 小样本学习或对可解释性要求高的场景         |
|                          Rule-Based                          | 具有很高的可解释性；不需要大量的数据进行训练；系统行为可预测 | 对于复杂的情况下系统会难以维护；对于未预见的新情况或变化的环境，适应能力差 | 规则确定或固定模式场景                     |
|                            TF-IDF                            |           速度快、资源要求低；可处理大规模文本数据           | 无法捕捉上下文和语义细微差异                                 | 资源受限环境、高并发请求、对延迟敏感的场景 |

