import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn


class CharLSTMDataset(Dataset):
    def __init__(self):
        dataset = pd.read_csv("./dataset.csv", sep='\t', header=None)
        self.texts = dataset[0].to_list()
        string_labels = dataset[1].to_list()

        self.label2index = {label: i for i, label in enumerate(set(string_labels))}
        self.num_labels = [self.label2index[label] for label in string_labels]
        self.num_labels = torch.tensor(self.num_labels, dtype=torch.long)

        self.char2index = {'<pad>': 0}
        for text in self.texts:
            for char in text:
                if char not in self.char2index:
                    self.char2index[char] = len(self.char2index)

        index2char = {i: char for char, i in self.char2index.items()}
        self.input_size = len(self.char2index)
        self.max_len = 40

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, index):
        text = self.texts[index]
        data = [self.char2index.get(char, 0) for char in text[:self.max_len]]
        data += [0] * (self.max_len - len(data))
        return torch.tensor(data, dtype=torch.long), self.num_labels[index]


class GRUNet(nn.Module):
    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):
        super(GRUNet, self).__init__()
        self.embedding = nn.Embedding(input_dim, embedding_dim)
        self.gru_layer = nn.GRU(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=1, batch_first=True)
        self.output_layer = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        
        x = self.embedding(x)  
        gru_output, h_n = self.gru_layer(x)  
        # h_n: [num_layers, batch_size, hidden_dim]
        x = self.output_layer(h_n[-1])  
        return x


def Train_model(embedding_dim, hidden_dim, num_epochs, dataset):
    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
    model = GRUNet(dataset.input_size, embedding_dim, hidden_dim, len(dataset.label2index))
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

    model.train()
    for epoch in range(num_epochs):
        for idx, (data, label) in enumerate(dataloader):
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, label)
            loss.backward()
            optimizer.step()
            if idx % 50 == 0:
                print(f"Epoch: {epoch + 1}/{num_epochs}, Batch num: {idx}, Current loss: {loss.item()}")
    print(f"Training finish!")
    return model


def classify_text(text, model, dataset, index2label):
    feature = [dataset.char2index.get(char, 0) for char in text[:dataset.max_len]]
    feature += [0] * (dataset.max_len - len(feature))
    input_tensor = torch.tensor(feature, dtype=torch.long).unsqueeze(0)  # 添加batch维度

    model.eval()
    with torch.no_grad():
        output = model(input_tensor)
    _, pre_label = torch.max(output, 1)
    pre_label = pre_label.item()
    pred_label = index2label[pre_label]

    return pred_label


if __name__ == '__main__':
    dataset = CharLSTMDataset()
    index2label = {i: label for label, i in dataset.label2index.items()}
    model = Train_model(64, 128, 4, dataset)

    new_text = "帮我导航到北京"
    pred_label = classify_text(new_text, model, dataset, index2label)
    print(f"input: {new_text}, pre_label: {pred_label}")
